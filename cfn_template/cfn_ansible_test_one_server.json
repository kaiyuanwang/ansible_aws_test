{"Description": "AWS Cloudformation template for TrueCall ansible test\n1 control, 2 slaves, can extend to more\nDependecies:\n  1. S3 bucket with pem file, packages, applications\n  2. IAM role for S3 access\n  3. Security group for My IP access only\n  4. Building of control server depends on successful built of slave servers\nFunctions:\n  1. build 1 control server and 2 slave servers with all application dependency packages \n  2. On control, install ansible, git, copy applicaiton packages from private s3 bucket\n  3. Use pem file from private s3 bucket for passwordless connections between all servers. change to rsa pair for enhanced security later.\n  4. Update contorl /etc/hosts with slave IPs\n  5. Need to update security group to allow laptop ip access\n  6. Use AWS cfn-init for cloudformation metadata control\n  7. Control server building depends on slave servers. Automatically roll back if control built fails.\n  8. Output server public IPs in cloudformation IP\n", "Resources": {"AnsibleTestSecurityGroup": {"Properties": {"GroupDescription": "Security group for Ansible Test Servers", "VpcId": {"Ref": "VpcId"}, "SecurityGroupIngress": [{"FromPort": "22", "IpProtocol": "tcp", "Description": "ssh", "ToPort": "22", "CidrIp": {"Ref": "SSHLocation"}}, {"FromPort": "80", "IpProtocol": "tcp", "Description": "http", "ToPort": "80", "CidrIp": "0.0.0.0/0"}, {"FromPort": "8443", "IpProtocol": "tcp", "Description": "https", "ToPort": "8443", "CidrIp": "0.0.0.0/0"}, {"FromPort": "9891", "IpProtocol": "tcp", "Description": "TrueCall", "ToPort": "9891", "CidrIp": "0.0.0.0/0"}]}, "Type": "AWS::EC2::SecurityGroup"}, "AnsibleTestSecurityGroupIngress": {"Properties": {"FromPort": "0", "ToPort": "65535", "IpProtocol": "tcp", "SourceSecurityGroupId": {"Ref": "AnsibleTestSecurityGroup"}, "GroupId": {"Ref": "AnsibleTestSecurityGroup"}}, "Type": "AWS::EC2::SecurityGroupIngress"}, "ControlHost": {"Properties": {"SecurityGroupIds": [{"Ref": "AnsibleTestSecurityGroup"}], "UserData": {"Fn::Base64": {"Fn::Sub": "#!/bin/bash -xe\n# Get the latest CloudFormation package\nyum update -y aws-cfn-bootstrap\n# Start cfn-init\n/opt/aws/bin/cfn-init -s ${AWS::StackId} -r ControlHost --region ${AWS::Region} || error_exit 'Failed to run cfn-init'\n# Start up the cfn-hup daemon to listen for changes to the EC2 instance metadata\n/opt/aws/bin/cfn-hup || error_exit 'Failed to start cfn-hup'\n\n\n# Install truecall dependencies\nyum groupinstall -y @core @debugging @development @hardware-monitoring @large-systems @performance @postgresql @security-tools @web-server @hardware-monitoring @large-systems @system-admin-tools @system-management @system-management-snmp --setopt=group_package_types=mandatory,default,optional\nyum install -y chrony python-psycopg2 httpd-tools libICE libSM libicu libyaml mailcap python-markupsafe perl-Archive-Zip protobuf libunwind sysfsutils lynx base install htop iftop ncurses-compat-libs postgresql postgresql-server mod_ssl\n\n# install git, ansible, pandas\nyum -y install git python-pip tree\namazon-linux-extras install ansible2 -y\npip install numpy pandas xlrd xlsxwriter\nyum update -y\ncd /opt/ansible\ngit init\n\n\n# Install RFB dependencies\nyum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\nyum install -y armadillo arpack atlas blas dbus git gzip libcap-ng libunwind lzma mcelog mlpack net-snmp-libs perl sshpass tbb zlib hdf5 lapack libgfortran libgnome-keyring libicu libquadmath\nwget http://mirror.centos.org/centos/7/extras/x86_64/Packages/libssh-0.7.1-3.el7.x86_64.rpm; yum localinstall -y libssh-0.7.1-3.el7.x86_64.rpm\n\n\n# download RFB related pkgs\nfor rfb_pkg in Netscout-FlowBroker-V7.17.3.0.3-0_rhel7.x86_64.rpm Netscout-FlowBroker-V7.17.4.0.3-1_cos731611el7.x86_64.rpm{,.md5} {xhhny82708_config.zip,JKT-RFB-01_config.tgz,boost_rpms.tgz} RFB_NE_TABLE_RRC_20181207.csv; do\n  aws s3 cp s3://ansible-test-kaiyuan/$rfb_pkg /opt\ndone\n#rpm -qa | grep boost | tr -s '\\n' ' '| xargs rpm -e\ntar xvf /opt/boost_rpms.tgz && yum -y localinstall boost*.rpm && rm -rf boost*.rpm\n\n\n# All done so signal success\n/opt/aws/bin/cfn-signal -e $? --stack ${AWS::StackId} --resource ControlHost --region ${AWS::Region}\n"}}, "InstanceType": {"Ref": "InstanceType"}, "KeyName": {"Ref": "KeyName"}, "IamInstanceProfile": {"Ref": "RoleName"}, "Tags": [{"Value": "Control", "Key": "Type"}], "ImageId": {"Fn::FindInMap": ["AWSRegionArch2AMI", {"Ref": "AWS::Region"}, {"Fn::FindInMap": ["AWSInstanceType2Arch", {"Ref": "InstanceType"}, "Arch"]}]}}, "Metadata": {"Comment": "1. Config passwordless connection to slaves. 2. Fetch packages. 3. Install ansible. 4. Install git. 5. YUM update.", "AWS::CloudFormation::Authentication": {"S3AccessCreds": {"buckets": ["ansible-test-kaiyuan"], "type": "S3", "roleName": {"Ref": "RoleName"}}}, "AWS::CloudFormation::Init": {"config": {"sources": {"/opt/config_ansible": {"Fn::Join": ["/", ["https://ansible-test-kaiyuan.s3.amazonaws.com", {"Ref": "ConfigAnsiblePkg"}]]}, "/opt": {"Fn::Join": ["/", ["https://ansible-test-kaiyuan.s3.amazonaws.com", {"Ref": "AnsiblePackage"}]]}, "/opt/ansible": {"Fn::Join": ["/", ["https://ansible-test-kaiyuan.s3.amazonaws.com", {"Ref": "ConfigAnsibleOutputConf"}]]}, "/opt/config_ansible/Truecall_backup": {"Fn::Join": ["/", ["https://ansible-test-kaiyuan.s3.amazonaws.com", {"Ref": "TruecallSiteBackup"}]]}, "/opt/tc3/scripts": {"Fn::Join": ["/", ["https://ansible-test-kaiyuan.s3.amazonaws.com", "tc_scripts.tar.gz"]]}}, "commands": {"config_ansible_connections": {"command": "sed -i \"s/production$/production_test/g\" /opt/ansible/ansible.cfg; sed -i \"s/^control/control ansible_connection=local/g\" /opt/ansible/production_test"}, "config_etc_hosts": {"command": "private_ip=`curl http://169.254.169.254/latest/meta-data/local-ipv4`; sed -i \"s/^control$/${private_ip} control/g\" /etc/hosts"}, "update_package_vars": {"command": "/opt/scripts/download_ansible_packages.sh"}, "config_ansible_privileges": {"command": "chmod 755 -R /opt/ansible\nchown -R ec2-user:ec2-user /opt/ansible\n"}}, "files": {"/opt/scripts/download_ansible_packages.sh": {"content": {"Fn::Sub": "aws s3 cp s3://ansible-test-kaiyuan/${TrueCallPackage} /opt/ansible/roles/tc_install/files\naws s3 cp s3://ansible-test-kaiyuan/${GsrPackage} /opt/ansible/roles/tc_install/files\n#sed -i \"s?^truecall_rpm:?truecall_rpm: ${TrueCallPackage}?; s?gsrsvcs_rpm:?gsrsvcs_rpm: ${GsrPackage}?g\" /opt/ansible/roles/tc_install/vars/main.yml\n####### TEST FOR test.ini.j2 JINJA2 TEMPLATE\nfor f in config.ini_vo{,.logic}.j2; do\n  #aws s3 cp s3://ansible-test-kaiyuan/$f /opt/ansible/roles/tc_install/templates\n  chmod 777 /opt/ansible/roles/tc_install/templates/$f\n  cp -p /opt/ansible/roles/tc_install/templates/`echo $f | sed 's/_vo//'` /opt/ansible/roles/tc_install/templates/`echo $f | sed 's/_vo//'`.back\n  mv /opt/ansible/roles/tc_install/templates/$f /opt/ansible/roles/tc_install/templates/`echo $f | sed 's/_vo//'`\ndone\nsed -i \"s/config.ini_vo.logic.j2/config.ini.logic.j2/g\" /opt/ansible/roles/tc_install/templates/config.ini.j2\n# task for test.ini.j2 test\n#mv /opt/ansible/roles/tc_install/tasks/main.yml /opt/ansible/roles/tc_install/tasks/main_org.yml\n#aws s3 cp s3://ansible-test-kaiyuan/test_main.yml /opt/ansible/roles/tc_install/tasks/main.yml\nmkdir /opt/ansible/group_vars/all/\n#aws s3 cp s3://ansible-test-kaiyuan/default.yml /opt/ansible/group_vars/all/\n#awk 'NR>1{print}' /opt/ansible/default.yml >> /opt/ansible/group_vars/TrueCall.yml\n#for f in {/opt/ansible/roles/tc_install/tasks/main.yml}; do\nchmod 777 /opt/ansible/roles/tc_install/tasks/main.yml\n#done\nchmod -R 777 /opt/config_ansible\n\ncat > /opt/ansible/roles/tc_install/files/tc_config_cleanup.sh << EOF\n#!/bin/bash\n\n# configure cylinderd server\nconfig_file=/opt/tc3/etc/config.ini;for i in `egrep \"LSR-server_COMMON_LTE|cylinderd_COMMON_LTE\" $config_file| awk -F/ '{print $1}'`; do sed -i \"/$i\\/args/s/$/|--disk-space-critical-threshold=2/g\" $config_file; done\necho \"alias tcdir='cd /var/lib/truecall/'\nalias tclog='cd /var/lib/truecall/log/'\nalias tcconfig='cd /opt/tc3/etc/'\nalias tcscript='cd /opt/tc3/scripts'\" >> ~root/.bashrc\nEOF\n\n# create TC related dirs, and download TC related pkgs\nmkdir -p /var/lib/truecall/COMMON_LTE/csv/190224/05 /opt/ansible/roles/tc_install/files /opt/tc3/scripts/\nchmod -R 777 /opt/tc3/scripts/\nfor f in handset_private.pem handset_public.pem handset_db.txt COMMON_LTE_201811140400_MELBOURNE_new.txt; do\n  touch /opt/ansible/roles/tc_install/files/$f\ndone\n\naws s3 cp s3://ansible-test-kaiyuan/2019022405.1550959240146450.csv.gz /var/lib/truecall/COMMON_LTE/csv/190224/05\n"}, "group": "root", "owner": "root", "mode": "000755"}, "/opt/scripts/ansible_commands.sh": {"content": {"Fn::Sub": "#!/bin/bash \n\nansible all -a \"sudo systemctl start truecall-server\"\nansible all -m shell -a 'ps -ef | grep tc3'\nansible all -m service -a \"name=truecall-server enabled=yes state=started\"\nansible all -m shell -a \"systemctl status truecall-server\"\nansible all -m yum -a \"name=truecall-server state=absent\"\nansible --list-hosts all\nansible-playbook --syntax-check tc_install.yml\nansible-playbook --check tc_install.yml\nansible-playbook -i production_test tc_install.yml -K --tags=\"stage,install\" -vvv\nansible-playbook -i production_test tc_install.yml -K --tags=\"reconfig\" -vvv\nansible control -m setup\nansible -m setup -a 'gather_subset=network' control\nansible -m debug -a 'var=hostvars[\"control\"]' control\nansible -m debug -a 'var=hostvars[\"slave_1\"]' control\nansible -m setup -a \"filter='ansible_all_ipv4_addresses'\" control\nansible-playbook -i production tc_report.yml -K --limits, --tags=\"lte\" -vvv\nansible-playbook -i production tc_upgrade.yml -K --tags=\"stage,upgrade\" --skip-tags=\"skipgsr\" -vvv\n\nyum remove truecall-server -y\n\n# configure cylinderd server\nconfig_file=/opt/tc3/etc/config.ini;for i in `egrep \"LSR-server_COMMON_LTE|cylinderd_COMMON_LTE\" $config_file| awk -F/ '{print $1}'`; do sed -i \"/$i\\/args/s/$/|--disk-space-critical-threshold=2/g\" $config_file; done\n"}, "group": "root", "owner": "root", "mode": "000755"}, "/opt/scripts/truecall_install.sh": {"content": {"Fn::Sub": "#!/bin/bash\n\n# Install truecall dependencies\n#yum groupinstall -y @core @debugging @development @hardware-monitoring @large-systems @performance @postgresql @security-tools @web-server @hardware-monitoring @large-systems @system-admin-tools @system-management @system-management-snmp --setopt=group_package_types=mandatory,default,optional\n#yum install -y chrony python-psycopg2 httpd-tools libICE libSM libicu libyaml mailcap python-markupsafe perl-Archive-Zip protobuf libunwind sysfsutils lynx base install htop iftop ncurses-compat-libs postgresql postgresql-server mod_ssl\n\n# truecall fresh install\ntc_pkg=/opt/ansible/roles/tc_install/files/TrueCall-Server-17.3.0.16-0-gaa28f3b-el7-x86_64.rpm; rpm -ivh $tc_pkg && rm -f $tc_pkg \ngsr_pkg=/opt/ansible/roles/tc_install/files/GSRservices-V7.17.3.0.3_PR_CP23-0_el7.x86_64.rpm; rpm -ivh $gsr_pkg && rm -f $gsr_pkg\nwait 10\nmv /opt/tc3/etc/config.ini /opt/tc3/etc/config.ini.org; cp -f /opt/tc3/scripts/one_server_config.ini /opt/tc3/etc/config.ini\ncp -f /opt/tc3/scripts/lte_local_settings.rb /opt/tc3/share/WebClient5/config/local_settings.rb\n/opt/tc3/share/WebClient5/conf-database.sh\n/opt/tc3/share/WebClient5/conf-webclient.sh\nrm -rf /opt/*.{tar.gz,rpm}\nsystemctl start truecall-server\nsystemctl enable truecall-server\n\n# configure http security\ngrep -q '^TraceEnable Off' /etc/httpd/conf/httpd.conf || echo \"TraceEnable Off \" >> /etc/httpd/conf/httpd.conf\ngrep -q \"RewriteCond %{REQUEST_METHOD} \\^TRACE\" /etc/httpd/conf.d/truecall.conf || sed -i '/DocumentRoot/a\\ RewriteEngine On\\n RewriteCond %{REQUEST_METHOD} ^TRACE\\n RewriteRule .* - [F]' /etc/httpd/conf.d/truecall.conf\nsystemctl reload httpd\n\n# configure crontab\ncp /opt/tc3/etc/crontab.pkg /opt/tc3/etc/crontab \n\n# config ne table and handset table\nsu - postgres -c \"pg_restore --dbname=tcadmin_production /opt/tc3/scripts/handset_database_setups_table_dump.sql\"\ncp -p /opt/tc3/scripts/handset_table.txt /var/lib/truecall/handset_database/uploads/tac_`date +%Y%m%d%H%M%S%3N`.txt\ncp -p /opt/tc3/scripts/VIC_TAS_COMMON_LTE_201806220033.txt /var/lib/truecall/ne_tables\n"}, "group": "root", "owner": "root", "mode": "000755"}, "/etc/cfn/hooks.d/cfn-auto-reloader.conf": {"content": {"Fn::Sub": "[cfn-auto-reloader-hook]\ntriggers=post.update\npath=Resources.ControlHost.Metadata.AWS::CloudFormation::Init\naction=/opt/aws/bin/cfn-init -v --stack ${AWS::StackName} --resource ControlHost --region ${AWS::Region}\n"}, "group": "root", "owner": "root", "mode": "000400"}, "/opt/scripts/run_configure_ansible.sh": {"content": {"Fn::Sub": "#!/bin/bash\n\n# config_ansible.py\n# generate configs\npython  config_ansible.py -G -s config_ansible_input/truecall_server_info_template.xlsx\n# update truecall_server_info.xlsx\npython configure_ansible.py -U\n# update config template from backup\npython config_ansible.py -P --backup-data-dir Truecall_backup_20181119\n"}, "group": "root", "owner": "root", "mode": "000755"}, "/opt/scripts/upload_tc_rfb_config.sh": {"content": {"Fn::Sub": "#!/bin/bash\n\nyum install -y dos2unix; dos2unix /opt/scripts/*sh /opt/tc3/scripts/*/*.{sh,py}\ntar czvf tc_install.tar.gz truecall_install.sh -C /opt/tc3/scripts/ {one_server_config.ini,lte_local_settings.rb,VIC_TAS_COMMON_LTE_201806220033.txt,handset_table.zip,handset_database_setups_table_dump.sql,database_backup.sql,databasedump_all.sql}\ntar czvf tc_scripts.tar.gz truecall_debug.sh -C /opt/tc3/scripts/ {one_server_config.ini,lte_local_settings.rb,VIC_TAS_COMMON_LTE_201806220033.txt,handset_table.zip,handset_database_setups_table_dump.sql,database_backup.sql,databasedump_all.sql,cell_converter,rrc_count_analysis,migrate_users,count_lsr}\ntar czvf tc_rfb_install_debug.tar.gz tc_install.tar.gz tc_scripts.tar.gz rfb_debug.sh rfb_install.sh\naws s3 cp tc_rfb_install_debug.tar.gz s3://ansible-test-kaiyuan/\naws s3 cp tc_scripts.tar.gz s3://ansible-test-kaiyuan/\n\ntar czvf opt_scripts.tar.gz -C /opt/scripts *.sh\naws s3 cp opt_scripts.tar.gz s3://ansible-test-kaiyuan/\n"}, "group": "root", "owner": "root", "mode": "000755"}, "/etc/hosts": {"content": {"Fn::Sub": "127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4 control\n::1         localhost6 localhost6.localdomain6\ncontrol\n"}, "group": "root", "owner": "root", "mode": "000755"}, "/opt/scripts/truecall_debug.sh": {"content": {"Fn::Sub": "#!/bin/bash\n\n# os debugging\ntop\nfree -h\nntpq -p\nless /var/log/messages\ndmesg\njournalctl -k\nmount | grep \"/var/lib/truecall\"\ndf \u2013hTP | grep \"/var/lib/truecall\"\nls /var/lib/truecall\ncat /etc/sysconfig/network-scripts/ifcfg-eth/eth0\nlsblk -o NAME,FSTYPE,LABEL,MOUNTPOINT,UUID,SIZE\ntracert localhost\ntelnet ip-of-server port-number\n/usr/bin/timeout 15 /usr/sbin/tcpdump -i any -c 5 udp dst port 54101\n\n# TCSTCP debugging\nwatch -n 1 \"ps -ef | grep tc3\"\nnetstat -natp | grep \u2013e \":7879.*LISTEN\"\nnetstat -anpl | grep process\n. /opt/tc3/bin/tc3-env.sh && processor-common-lte --help | less\n\n# tc-cmd\n. /opt/tc3/bin/tc3-env.sh && tc-cmd localhost 9891 user admin 123456 mega_report all_kpi_explorer time  2019-02-23T00:00:00.000 2019-02-23T00:00:00.001\n\n# postgres database\nservice postgresql status\nsu postgres -c \"psql tcadmin_production\"\n  # select * from users;\n  # \\d \\h \\?\nsu - postgres -c \"pg_dump tcadmin_production -FC -t users -t user_user_groups -t user_groups -t user_permissions -t user_group_user_permissions -t handset_database_setups > /tmp/handset_database_setups_table_dump.sql\"\nsu - postgres -c \"pg_dump tcadmin_production -FC > /tmp/database_backup.sql\"\nsu - postgres -c \"pg_dumpall > /tmp/databasedump_all.sql\"\nsu - postgres -c \"pg_restore --dbname=tcadmin_production /opt/tc3/scripts/handset_database_setups_table_dump.sql\"\nsu - postgres -c \"pg_restore --dbname=tcadmin_production /opt/tc3/scripts/database_backup.sql\"\nsu - postgres -c \"psql < /opt/tc3/scripts/databasedump_all.sql\"\n# first delete related tables, then import\nfor dump in *.dump; do table_name=`echo $dump | awk -F. '{print $1}'`;su - postgres -c \"psql -d tcadmin_production -c 'truncate $table_name';pg_restore  --dbname=tcadmin_production /`pwd`/$dump\"; done\n\n# NE table and handset database debugging, between ETL/LSR and TCSTCP\ntree /var/lib/truecall/ne_tables/\nsu - postgres -c \"psql -d tcadmin_production -c 'select * from lte_uploads'\" > test.txt \nsu - postgres -c \"psql -d tcadmin_production -c 'select * from lte_network_elements\u2019\u201d > test.txt\nsu - postgres -c \"psql -d tcadmin_production -c 'select * from lte_network_element_tables'\" >test.txt\nless /var/lib/truecall/handset_database/3GPP/active/devmap.tsv\n\n# LSR debugging\nwatch -n 1  \"ps -flwwu daemon | grep -i lsr\"\nntpq -p\ngrep -i error /var/lib/truecall/log/LSR-server_COMMON_LTE.log\nnetstat -anpl | grep -i lsr\nstat /var/lib/truecall/COMMON_LTE/csv_tmp\ndu -sh /var/lib/truecall/COMMON_LTE/csv/190224/05\nstat /var/lib/truecall/COMMON_LTE/csv/190224/05\n\n# check column name\ncd /var/lib/truecall/COMMON_LTE/csv/190224/05; zcat 2019022405.1550959240146450.csv.gz | head -1 | tr -s ',' '\\n' | nl\n# check rrc count\nzcat 2019022405.1550959240146450.csv.gz | awk -F',' '{print $21}' | sort | uniq -c | awk '{if($2==\"\")print $1}'\necho \"imsi,rrc,erab\"; zcat 2019022405.1550959240146450.csv.gz | awk -F, 'NR>1{if($56==\"False\"){cell[$14]++; erab_count[$14]+=$57; if($11==\"New Connection\") rrc_count[$14]++}} END{n=asorti(cell,cell_sorted); for(i=1;i<=n;i++)print cell_sorted[i]\",\"rrc_count[cell_sorted[i]]\",\"erab_count[cell_sorted[i]]}'\n"}, "group": "root", "owner": "root", "mode": "000755"}, "/home/ec2-user/.ssh/config": {"content": "Host control\nIdentityFile ~/.ssh/MyEC2KeyPair.pem\n", "group": "ec2-user", "owner": "ec2-user", "mode": "000400"}, "/etc/cfn/cfn-hup.conf": {"content": {"Fn::Sub": "[main]\nstack=${AWS::StackId}\nregion=${AWS::Region}\n"}, "group": "root", "owner": "root", "mode": "000400"}, "/opt/scripts/rfb_debug.sh": {"content": {"Fn::Sub": "#!/bin/bash\n\n# os debugging\njournalctl\njournalctl -k\ntop\nfree -h\nntpq -p\nless /var/log/messages\ndmesg\n\n# rfb debugging\nyum info Netscout-FlowBroker\n/etc/cron.hourly/fbAutocluster.cron\n/opt/netscout-fb/rtm/rfbinfo -r # -> /root/rfb_info_ip-172-31-15-223.ap-southeast-2.compute.internal_2019-02-24-07.33.36+0000.tar.gz\n# rfb network debugging\ntraceroute -T -p 22227 localhost\n/usr/bin/timeout 15 /usr/sbin/tcpdump -i any -c 5 udp dst port 22227\n\n# fbcmd debugging\n/opt/netscout-fb/rtm/fbcmd -l 22227 -c \"showreceivers\"\n/opt/netscout-fb/rtm/fbcmd -l 22227 -c \"showactivesenders\"\n/opt/netscout-fb/rtm/fbcmd -l 22223 -c \"tpm: showenodebstats 13\"\n\n# rfb configurations\n# markets in regions.yml\ngrep market: /opt/netscout-fb/standalone/config/regions.yml | sort | uniq\n# regions under each market\ngrep '\\-\\ \\&' /opt/netscout-fb/standalone/config/regions.yml\n# markets feeding one fbServer instance\ngrep MarketId /opt/netscout-fb/standalone/etc/fbServer_SA.conf\n# gsrClusters for one fbServer instance. should be 1:1 mapping to regions in one market\ngrep gsrCluster /opt/netscout-fb/standalone/config/gsrclusters_WA.yml\n\n# git health check and rollback\ncd /opt/netscout-fb/standalone/config\ngit log # -> git diff <commit_1> <commit_2>\ngit show <commit>:regions.yml >> ~/regions.yml\n# soft rollback\ncp ~/regions.yml /opt/netscout-fb/NETables\n\n# process debugging and core file generation\n# strace -p <pid>, gdb -p <pid>\nps -ef | grep [f]bServer.*22227 | awk '{print $2}' | xargs strace -p\nps -ef | grep [f]bServer.*22227 | awk '{print $2}' | xargs gdb -p\n\n# coredump\nsudo su -\nmkdir /tmp/cores; chmod 777 /tmp/cores\necho \"/tmp/cores/core.%e.%p.%h.%t\" > /proc/sys/kernel/core_pattern\nadd the line \"ulimit -c unlimited\" before exec command\n"}, "group": "root", "owner": "root", "mode": "000755"}, "/opt/scripts/rfb_install.sh": {"content": {"Fn::Sub": "#!/bin/bash\n\n# Install RFB dependencies\n#yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n#yum install -y armadillo arpack atlas blas dbus git gzip libcap-ng libunwind lzma mcelog mlpack net-snmp-libs perl sshpass tbb zlib hdf5 lapack libgfortran libgnome-keyring libicu libquadmath\n#wget http://mirror.centos.org/centos/7/extras/x86_64/Packages/libssh-0.7.1-3.el7.x86_64.rpm; yum localinstall -y libssh-0.7.1-3.el7.x86_64.rpm\n\n# validate RFB rpm with md5\nrpm -K Netscout-FlowBroker-V7.17.4.0.3-1_cos731611el7.x86_64.rpm\n\n# install RFB rpm\nyum localinstall --nogpgcheck -y Netscout-FlowBroker-V7.17.4.0.3-1_cos731611el7.x86_64.rpm\nunzip -o /opt/xhhny82708_config.zip -d /opt/netscout-fb/standalone\n#tar xvf /opt/JKT-RFB-01_config.tgz -C /opt/netscout-fb/standalone\nmkdir /opt/netscout-fb/NETables; cp /opt/RFB_NE_TABLE_RRC_20181207.csv /opt/netscout-fb/NETables\nsystemctl start fbMon\n/opt/netscout-fb/rtm/runfbautocluster 2>&1 >> /opt/netscout-fb/log/fbAutocluster.log\n"}, "group": "root", "owner": "root", "mode": "000755"}, "/home/ec2-user/.ssh/MyEC2KeyPair.pem": {"source": "https://ansible-test-kaiyuan.s3.amazonaws.com/MyEC2KeyPair.pem", "owner": "ec2-user", "mode": "000400", "group": "ec2-user"}}}}}, "Description": "Ansible control host", "CreationPolicy": {"ResourceSignal": {"Timeout": "PT15M"}}, "Type": "AWS::EC2::Instance"}}, "Outputs": {"ControlPublicIp": {"Description": "Public IP address of the control server", "Value": {"Fn::GetAtt": ["ControlHost", "PublicIp"]}}}, "Parameters": {"RoleName": {"Default": "S3-Admin-Access", "Description": "Name of IAM Role for EC2 instances", "ConstraintDescription": "must be the name of an existing IAM Role.", "Type": "String"}, "GsrPackage": {"Default": "GSRservices-V7.17.3.0.3_PR_CP23-0_el7.x86_64.rpm", "Description": "Gsr package name in s3://ansible-test-kaiyuan bucket", "ConstraintDescription": "Available Gsr package file.", "Type": "String"}, "ConfigAnsiblePkg": {"Default": "config_ansible.zip", "Description": "Software package of config_ansible.py in s3://ansible-test-kaiyuan bucket", "ConstraintDescription": "Available Ansible Configuration file pkg.", "Type": "String"}, "ConfigAnsibleOutputConf": {"Default": "config_ansible_output_one_server.zip", "Description": "Ansible Configuration files in s3://ansible-test-kaiyuan bucket. Generated by config_ansible.py. Available options are 1 server config_ansible_output_one_server.zip, and 3 servers config_ansible_output_aws.zip", "ConstraintDescription": "Available Ansible Configuration files.", "Type": "String"}, "SSHLocation": {"AllowedPattern": "(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2})", "Description": "The IP address range that can be used to SSH to the EC2 instances", "Type": "String", "MaxLength": "18", "ConstraintDescription": "must be a valid IP CIDR range of the form x.x.x.x/x.", "MinLength": "9"}, "TrueCallPackage": {"Default": "TrueCall-Server-17.3.0.16-0-gaa28f3b-el7-x86_64.rpm", "Description": "TrueCall package name in s3://ansible-test-kaiyuan bucket", "ConstraintDescription": "Available TrueCall package,", "Type": "String"}, "VpcId": {"Default": "vpc-0fa16d5fa89c52bd9", "Description": "VpcId of your existing Virtual Private Cloud (VPC)", "Type": "AWS::EC2::VPC::Id"}, "AnsiblePackage": {"Default": "ansible_truecall_v17.3_02Nov18.tar.gz", "Description": "ansible package name in s3://ansible-test-kaiyuan bucket", "ConstraintDescription": "Available ansible package ,", "Type": "String"}, "InstanceType": {"Default": "t2.micro", "Description": "EC2 instance type", "AllowedValues": ["t1.micro", "t2.nano", "t2.micro", "t2.small", "t2.medium", "t2.large", "m1.small", "m1.medium", "m1.large", "m1.xlarge", "m2.xlarge", "m2.2xlarge", "m2.4xlarge", "m3.medium", "m3.large", "m3.xlarge", "m3.2xlarge", "m4.large", "m4.xlarge", "m4.2xlarge", "m4.4xlarge", "m4.10xlarge", "c1.medium", "c1.xlarge", "c3.large", "c3.xlarge", "c3.2xlarge", "c3.4xlarge", "c3.8xlarge", "c4.large", "c4.xlarge", "c4.2xlarge", "c4.4xlarge", "c4.8xlarge", "g2.2xlarge", "g2.8xlarge", "r3.large", "r3.xlarge", "r3.2xlarge", "r3.4xlarge", "r3.8xlarge", "i2.xlarge", "i2.2xlarge", "i2.4xlarge", "i2.8xlarge", "d2.xlarge", "d2.2xlarge", "d2.4xlarge", "d2.8xlarge", "hi1.4xlarge", "hs1.8xlarge", "cr1.8xlarge", "cc2.8xlarge", "cg1.4xlarge"], "ConstraintDescription": "must be a valid EC2 instance type.", "Type": "String"}, "KeyName": {"Default": "MyEC2KeyPair", "Description": "Name of an existing EC2 KeyPair to enable SSH access to the instances", "ConstraintDescription": "must be the name of an existing EC2 KeyPair.", "Type": "AWS::EC2::KeyPair::KeyName"}, "TruecallSiteBackup": {"Default": "Truecall_backup_20181119.zip", "Description": "Site backups of TrueCall servers in s3://ansible-test-kaiyuan bucket", "ConstraintDescription": "Available TrueCall site backup demos.", "Type": "String"}}, "Metadata": {"AWS::CloudFormation::Interface": {"ParameterGroups": [{"Parameters": ["SSHLocation", "VpcId", "InstanceType"], "Label": {"default": "EC2 Configuration"}}, {"Parameters": ["KeyName", "RoleName"], "Label": {"default": "Connection Configuration"}}, {"Parameters": ["AnsiblePackage", "TrueCallPackage", "GsrPackage", "ConfigAnsibleOutputConf", "ConfigAnsiblePkg", "TruecallSiteBackup"], "Label": {"default": "Ansible Configuration"}}]}}, "AWSTemplateFormatVersion": "2010-09-09", "Mappings": {"AWSInstanceType2Arch": {"t2.nano": {"Arch": "HVM64"}, "t2.small": {"Arch": "HVM64"}, "t2.micro": {"Arch": "HVM64"}, "t2.large": {"Arch": "HVM64"}, "t2.medium": {"Arch": "HVM64"}}, "AWSRegionArch2AMI": {"ap-southeast-2": {"HVM64": "ami-08589eca6dcc9b39c"}}}}